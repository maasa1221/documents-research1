### 1.SVM(サポートベクターマシン）とカーネル

もっとも有名であろう機械学習ライブラリ scikit-learn を用いる。  
scikit-learn に搭載されている SVM 分類器　 sklearn.svm.SVC=> Support Vector Classification.
これらには多種多様なカーネルと呼ばれるものが搭載されている。
？？？？？OS のカーネルではないのか？一体カーネルとはなんなのか？その謎の追及にも迫る。

カーネル(OS）->

オペレーティングシステムの基本コンポーネントとして、カーネルはメモリ、CPU、入出力を中心としたハードウェアを抽象化し、ハードウェアとソフトウェアがやり取りできるようにする。また、ユーザープログラムのための機能として、プロセスの抽象化、プロセス間通信、システムコールなどを提供する。

カーネル（機械学習）->

カーネルトリック!!!!???

理解を進めるにつれて後記のカーネルは特徴量エンジニアリング(主成分分析など）に近しいものであるということがわかった。
分類や回帰、クラスタリングなど色々ある機械学習手法を用いてデータから何かしらのヒントを得ようと思った時に必要なのは、そのデータがどのようなデータであるかを知っておくことであろう。学習にかける前に人間側からあらかじめわかりやすいように加工を施してあげることにより学習の制度を向上させる。これは特徴量エンジニアリングにおいて前処理と呼ばれ、標準化、正規化、主成分分析、そしてグラフによる可視化もこの工程に入るかもしれない。
デジタルコンペ Kaggle 的な言い方で言うと EDA(データの理解)の後に行う作業である。

http://enakai00.hatenablog.com/entry/2017/10/13/145337
↓
『データが本来的にもっているであろう性質やら、過去の試行錯誤の経験を元に、手探りで特徴量エンジニアリングを進めていくことになります。』
結局、前処理や EDA などの特徴量エンジニアリングは人間自らが行わなければならないもの、ここをめんどくせえええと思った先人がいたのか？
![](20171013103327.png)
![](2019-10-21-16-03-30.png)
たとえば、上のような左図の場合（x,y）平面で表現されているに限っては分類はめんどくさそうに見えます。
しかし、右のように新たな z 軸を設けてあげることにより分類を容易にしています。そう、(x,y)の平面から(x,y,z)の局面へと変換していると言うことになります。
この場合であれば z=x2+y2 を追加することによりそれを実現しています。

これを念頭に入れて少し戻ります。
そもそもなぜ、特徴量エンジニアリングは必要であるのか？と言うことです。
その理由は機械学習が**多数の特徴量を用意して、それらをモデルに入力**すると言うプロセスをとるからでしょう。
例えば、
X={x1,x2,⋯,xK}
X と言う変数があったとします。一直線上に点が分散しているイメージ

これに対して
ϕ={ϕ1(x),ϕ2(x),⋯,ϕM(x)}
ϕ1(x)=x21 とか ϕ2(x)=x1x2 があるとします。

X が K 個あったとして ϕ は X 内の組み合わせの数だけ存在するはずなので K<M が成り立つでしょう。つまるところ、膨大な数の組み合わせ計算を用意しているのです。
さっきの(x,y,z)の図でやっていたことと同じことをしようと本質的にはしているのですが、あちらの場合と異なり現実の問題では M 個の中から Z に相当するものを見付け出さなければならないと言うことです。しかし、これらの中からどうやって Z を見つけ出せばいいのか？これが大変重要な問題でしょう。M 個全てを Z(遅くなったがこの場合は y)に代入してもっともスコアの高いものを抽出するのか？途方にくれるやもしれません。

ここで遂にカーネルトリックと言う言葉が登場します。
![20171013103327.png](attachment:20171013103327.png)

例えば、この上図の場合、(x,y)の二次元空間を z を含む新たな３次元空間に写像することにより、超平面による分離を可能にしています。
この導出に至る流れをカーネルトリックと呼びます。
K(x1,x2)=φx1・φx2
φx1 は x1 と言う点を新たに写像した結果として出てきた点をさします。
そして K(x1,x2)はこれら二つの内積を表します。内積はある意味ではベクトルの類似度を表しているといえます。どれだけ同じ方向を向いているのかと言う指標にもなりえます。この場合であれば、原点（０、０、0）からの３次元ベクトルになっており、点同士の内積は原点から見て点同士がどれくらい近しいいちに存在しているかの指標になります。

この K(x1,x2)=φx1・φx2 をカーネル関数と呼びます。
ここで、あれ？となるのは K は引数として φx1 と φx2 を直接もらっていないと言うことです。K が引数として受け取っているのは写像後のベクトルではなく、写像前のベクトルになっています。これが味噌であり、もっとも面倒な適切な写像関数を導出せずとも分離に導ける方であると言えます。これをカーネルトリックと呼びます。
このカーネルトリックの種類として、
Polynomial kernel(多項式カーネル）
Gaussian 　 kernel(ガウシアンカーネル)
後述する
Linear kernel（線形カーネル）
Radial Basis function kernel(動径基底関数カーネル)
Precomputed kernel
Sigmoid kernel
などなど色々あります。
結論としてカーネルの意義は線形分離が不可能なデータ群を高次元空間に写像し非線形分離を可能にするための方法であった。
![%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88%202019-10-19%2015.09.00.png](attachment:%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88%202019-10-19%2015.09.00.png)

X={x1,x2,⋯,xK}　 ϕX={ϕ1(x),ϕ2(x),⋯,ϕM(x)} K 次元データ X を M 次元データ φX に写像する関数 φ とはなんなのかと言うことを突き詰めて汎用性を向上させる必要性があるだろう。

これらデータを実際に単純パーセプトロンを用いて実装した手法として SVM が存在する。
単純パーセプトロンの次元数 W を最適化する各サポートベクターのマージン最大化があげられる。
![%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88%202019-10-19%2015.33.06.png](attachment:%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88%202019-10-19%2015.33.06.png)

下図式より
識別結果 f(x)は重みパラメータ W の転置式と入力ベクトル x

![%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88%202019-10-19%2015.39.06.png](attachment:%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88%202019-10-19%2015.39.06.png)

多変量解析と言う言葉にであった。
多変量解析とは何か？
多変量解析には主成分分析、重回帰分析、クラスター分析などありそこにカーネル法も含まれている。https://www.sist.ac.jp/~suganuma/kougi/other_lecture/SE/multi/multi.html より、多変量解析は主成分分析などに代表されるように多くの変数（変量）をもつデータに対して目的に応じた統合を促すための方法である。簡単に言えば生データを機械学習などで取り扱いやすくするための手法である。これらの多変量解析手法は生データの特性とその使用目的に応じて使い分けられる。